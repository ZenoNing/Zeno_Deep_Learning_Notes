# Issues

## Issue1 MAT's training process

<div align="center"><img src="https://raw.githubusercontent.com/ZenoNing/Zeno_Deep_Learning_Notes/main/2024/Architecture_LaMa.png"></div>

Training this model  requires high performance computational resources. According to the Author's statement in their GitHub's repository [<sup>1</sup>](#refer-anchor-1) , training on CelebA-HQ512 datasets [<sup>1</sup>](#refer-anchor-2) (which contains 24,813 training images) 

<div align="center"><img src="https://raw.githubusercontent.com/ZenoNing/Zeno_Deep_Learning_Notes/main/2024/Architecture_MAT.png"></div>

<div align="center"><img src="https://raw.githubusercontent.com/ZenoNing/Zeno_Deep_Learning_Notes/main/2024/Architecture_RePaint.png"></div>


# References

<div id="refer-anchor-1"></div>

- [1] [MAT: Mask-Aware Transformer for Large Hole Image Inpainting](https://github.com/fenglinglwb/MAT/issues/23)
